{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import packages and define functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [],
   "source": [
    "import arcpy\n",
    "import os\n",
    "import tempfile\n",
    "import glob\n",
    "import datetime\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Function for spatial joins\n",
    "def execute_spatial_joins(point_layer, input_wbd_layer, source, lake_id_field_name, search_radius_meters, output_point_layer_path):\n",
    "    input_wbd_layer_name = str(source) + os.path.basename(input_wbd_layer).replace('.shp', '')\n",
    "    search_radius_value = str(search_radius_meters) + \" Meters\"\n",
    "\n",
    "    print(f\"Processing: {input_wbd_layer_name}\")\n",
    "\n",
    "    # Define the path for the temporary layer for direct intersection\n",
    "    temp_layer = os.path.join(temp_gdb, f'{input_wbd_layer_name}_temp_layer') ## EDK: Variable temp_gdb and also 'point_intersections' apparently undefined, But somehow the code still worked.\n",
    "    # Define the path for the temporary layer for neighborhood spatial join\n",
    "    dist_temp_layer = os.path.join(temp_gdb, f'{input_wbd_layer_name}_dist_temp_layer')\n",
    "    # Define the path for the temporary layer for nearest features\n",
    "    near_temp_layer = os.path.join(temp_gdb, f'{input_wbd_layer_name}_near_temp_layer')\n",
    "\n",
    "\n",
    "    # Perform spatial join to identify points directly intersecting lakes\n",
    "    arcpy.analysis.SpatialJoin(\n",
    "        target_features=point_layer,\n",
    "        join_features=input_wbd_layer,\n",
    "        out_feature_class=temp_layer,\n",
    "        join_operation=\"JOIN_ONE_TO_MANY\",\n",
    "        join_type=\"KEEP_COMMON\",\n",
    "        match_option=\"INTERSECT\",\n",
    "        search_radius=None,\n",
    "        distance_field_name=\"\"\n",
    "    )\n",
    "    \n",
    "\n",
    "    # Record the total number of lakes that each point intersects with. Record the IDs of the lakes each point intersects with. \n",
    "    with arcpy.da.SearchCursor(temp_layer, [\"TARGET_FID\", \"JOIN_FID\", str(lake_id_field_name)]) as cursor:\n",
    "        for row in cursor: # EDK: Might these row indexes change if you repeat this operation on a future version of this file that has more attributes or a different attribute order? Or maybe you have no choice and this is an arcpy quirk.\n",
    "            point_fid = row[0]\n",
    "            polygon_fid = row[1]\n",
    "            lake_id = row[2]\n",
    "            \n",
    "            new_polygon_id = str(input_wbd_layer_name)+\"_\"+str(polygon_fid)+\"_\"+str(lake_id) \n",
    "            \n",
    "            if point_fid not in point_intersections:\n",
    "                point_intersections[point_fid] = {\"count\": 0, \"ids\": [], \"dist_count\": 0, \"dist_ids\": [], \"near_id\": [], \"ndist_m\": []}\n",
    "            point_intersections[point_fid][\"count\"] += 1\n",
    "            point_intersections[point_fid][\"ids\"].append(str(new_polygon_id))\n",
    "\n",
    "            \n",
    "\n",
    "    # Perform spatial join to identify lakes within 100 m of points\n",
    "    arcpy.analysis.SpatialJoin( # EDK: Here is where arcGIS has a better alg than QGIS. in QGIS, Spatial join within a distance requires creating a buffered version of the joint features. This in turn, required projecting to an equal area coordinate system.\n",
    "        target_features=point_layer,\n",
    "        join_features=input_wbd_layer,\n",
    "        out_feature_class=dist_temp_layer,\n",
    "        join_operation=\"JOIN_ONE_TO_MANY\",\n",
    "        join_type=\"KEEP_COMMON\",\n",
    "        match_option=\"WITHIN_A_DISTANCE_GEODESIC\",\n",
    "        search_radius=search_radius_value,\n",
    "        distance_field_name=\"\"\n",
    "    )\n",
    "\n",
    "    # Record the total number of lakes that each point is within 100 m of. Record the IDs of the lakes within 100 m of each point. ## EDK can this snippet be generalized to a function?\n",
    "    with arcpy.da.SearchCursor(dist_temp_layer, [\"TARGET_FID\", \"JOIN_FID\", str(lake_id_field_name)]) as cursor:\n",
    "        for row in cursor:\n",
    "            dist_point_fid = row[0]\n",
    "            dist_polygon_fid = row[1]\n",
    "            dist_lake_id = row[2]\n",
    "            \n",
    "            new_dist_polygon_id = str(input_wbd_layer_name)+\"_\"+str(dist_polygon_fid)+\"_\"+str(dist_lake_id) \n",
    "            \n",
    "            if dist_point_fid not in point_intersections:\n",
    "                point_intersections[dist_point_fid] = {\"count\": 0, \"ids\": [], \"dist_count\": 0, \"dist_ids\": [], \"near_id\": [], \"ndist_m\": []}\n",
    "            point_intersections[dist_point_fid][\"dist_count\"] += 1\n",
    "            point_intersections[dist_point_fid][\"dist_ids\"].append(str(new_dist_polygon_id)) # EDK: I wonder if this string of potentialy > 1 polygon IDs will be easier to parse, Or if you should use \"tall\" Data format where each point can appear in multiple rows, one for each polygon match. Just a thought â€“ obviously the current method works for you.\n",
    "\n",
    "    search_radius_value = str(search_radius_meters) + \" Meters\"\n",
    "\n",
    "    # Generate Near Table to identify closest lakes within 100 m of points\n",
    "    arcpy.analysis.GenerateNearTable(\n",
    "        in_features=point_layer,\n",
    "        near_features=input_wbd_layer,\n",
    "        out_table=near_temp_layer,\n",
    "        search_radius=search_radius_value,\n",
    "        location=\"NO_LOCATION\",\n",
    "        angle=\"NO_ANGLE\",\n",
    "        closest=\"CLOSEST\",\n",
    "        closest_count=1, # EDK: I think this parameter is ignored if you have specified closest.\n",
    "        method=\"GEODESIC\",\n",
    "        distance_unit=\"Meters\"\n",
    "    )\n",
    "    \n",
    "    # Create dictionary that lists lake IDs for each near_polygon_fid # EDK: Here is where I would recommend creating a common field name between Water body datasets so you can remove this if statement.\n",
    "    if source == \"PLD\":\n",
    "        identifier_field = \"FID\"\n",
    "    else:\n",
    "        identifier_field = \"OBJECTID\"\n",
    "        \n",
    "    near_fields = [str(identifier_field), str(lake_id_field_name)]\n",
    "    near_features = {}\n",
    "    with arcpy.da.SearchCursor(input_wbd_layer, near_fields) as cursor:\n",
    "        for row in cursor:\n",
    "            near_polygon_fid = row[0]\n",
    "            near_lake_id = row[1]\n",
    "            \n",
    "            new_near_polygon_id = str(input_wbd_layer_name)+\"_\"+str(near_polygon_fid)+\"_\"+str(near_lake_id) \n",
    "            \n",
    "            near_features[near_polygon_fid] = {\"new_lake_id\": new_near_polygon_id}\n",
    "            \n",
    "    # Record the nearest lake ID and distance (within 100 m) for each point\n",
    "    with arcpy.da.SearchCursor(near_temp_layer, [\"IN_FID\", \"NEAR_FID\", \"NEAR_DIST\"]) as cursor:\n",
    "        for row in cursor:\n",
    "            near_point_fid = row[0]\n",
    "            near_join_fid = row[1]\n",
    "            near_distance = row[2]\n",
    "            \n",
    "            if near_point_fid not in point_intersections:\n",
    "                point_intersections[near_point_fid] = {\"count\": 0, \"ids\": [], \"dist_count\": 0, \"dist_ids\": [], \"near_id\": [], \"ndist_m\": []} # EDK: This snippet Could also be generalized into a function, where you supply the names \"near\", \"dist\" and \"\" when you call it\n",
    "            else:\n",
    "                point_intersections[near_point_fid][\"near_id\"].append(str(near_features[near_join_fid][\"new_lake_id\"]))\n",
    "                point_intersections[near_point_fid][\"ndist_m\"].append(str(near_distance))\n",
    "\n",
    "    return point_intersections # EDK: I'd like to see this output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set base parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-07-28\n"
     ]
    }
   ],
   "source": [
    "# Define basepath\n",
    "basepath =  r\"D:\\__THOWARD\\ABOVE_Project\\Technical\"\n",
    "todays_date = datetime.date.today().strftime('%Y-%m-%d') # EDK: Good version control!\n",
    "print(todays_date)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class='gpresult'><h2>Messages</h2><div id='messages' data-messages='[\"Start Time: Sunday, July 28, 2024 6:52:29 PM\",\"Succeeded at Sunday, July 28, 2024 6:52:29 PM (Elapsed Time: 0.13 seconds)\"]' data-show='true'><div id = 'default' /></div></div>"
      ],
      "text/plain": [
       "<Result 'D:\\\\__THOWARD\\\\ABOVE_Project\\\\Technical\\\\InputData\\\\Vectors\\\\LAKESHAPE\\\\LAKESHAPE\\\\effluxlakes.shp'>"
      ]
     },
     "execution_count": 265,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Path to point feature layer\n",
    "point_layer_path = os.path.join(basepath, r\"InputData\\Vectors\\LAKESHAPE\\LAKESHAPE\\effluxlakes.shp\")\n",
    "\n",
    "# Path to the output point feature layer\n",
    "output_point_layer_path = os.path.join(basepath, r\"OutputData\\Vectors\\PointFeatureSpatialJoins\\effluxlakes_spatialjoins_{}.shp\".format(todays_date))\n",
    "\n",
    "# Copy point layer\n",
    "arcpy.management.Copy(point_layer_path, output_point_layer_path)\n",
    "\n",
    "# Create a spatial index for the point layer\n",
    "arcpy.management.AddSpatialIndex(point_layer_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conduct Spatial Joins with PLD Lakes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class='gpresult'><h2>Messages</h2><div id='messages' data-messages='[\"Start Time: Sunday, July 28, 2024 6:52:35 PM\",\"Adding PLDNdist_m to effluxlakes_spatialjoins_2024-07-28...\",\"Succeeded at Sunday, July 28, 2024 6:52:36 PM (Elapsed Time: 0.20 seconds)\"]' data-show='true'><div id = 'default' /></div></div>"
      ],
      "text/plain": [
       "<Result 'D:\\\\__THOWARD\\\\ABOVE_Project\\\\Technical\\\\OutputData\\\\Vectors\\\\PointFeatureSpatialJoins\\\\effluxlakes_spatialjoins_2024-07-28.shp'>"
      ]
     },
     "execution_count": 266,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Path to folder containing shapefiles of lake polygons\n",
    "PLD_shapefile_folder = os.path.join(basepath, r\"InputData\\Vectors\\PLD_TopoCat_dataset\\PLD_TopoCat_dataset\\PLD_lakes\")\n",
    "\n",
    "# List of all the shapefiles in the shapefile folder\n",
    "PLD_shapefiles = glob.glob(os.path.join(PLD_shapefile_folder, '*.shp')) # EDK: If this loop over files is taking a long time or requires a lot of lines of code, perhaps you can merge these individual shaped files into a GDB (which doesn't have a feature limit like SHP)? The best tool for this is ogr2ogr on the command line, and should take up to ~20 minutes.\n",
    "\n",
    "# Reduce number of shapefiles searched\n",
    "###### ETHAN, CAN YOU PLEASE LET ME KNOW IF YOU THINK THAT I HAVE MISSED A SHAPEFILE?\n",
    "endings = [\n",
    "    '_71.shp',\n",
    "    '_72.shp',\n",
    "    '_74.shp',\n",
    "    '_78.shp',    \n",
    "    '_81.shp',    \n",
    "    '_82.shp',    \n",
    "    '_83.shp',    \n",
    "    '_84.shp',    \n",
    "    '_85.shp',\n",
    "    '_86.shp', \n",
    "    '_91.shp'\n",
    "]\n",
    "\n",
    "PLD_shapefile_list = [shp for shp in PLD_shapefiles if any(shp.endswith(ending) for ending in endings)]\n",
    "\n",
    "# Add new columns to the point layer for PLD \n",
    "arcpy.management.AddField(output_point_layer_path, 'PLDIntNum', 'LONG')\n",
    "arcpy.management.AddField(output_point_layer_path, 'PLDIntIDs', 'TEXT', field_length=500)\n",
    "arcpy.management.AddField(output_point_layer_path, 'PLD100mNum', 'LONG')\n",
    "arcpy.management.AddField(output_point_layer_path, 'PLD100mIDs', 'TEXT', field_length=500)\n",
    "arcpy.management.AddField(output_point_layer_path, 'PLDNdistID', 'TEXT', field_length=500)\n",
    "arcpy.management.AddField(output_point_layer_path, 'PLDNdist_m', 'TEXT', field_length=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class='gpresult'><h2>Messages</h2><div id='messages' data-messages='[\"Start Time: Sunday, July 28, 2024 6:52:39 PM\",\"Succeeded at Sunday, July 28, 2024 6:52:40 PM (Elapsed Time: 0.22 seconds)\"]' data-show='true'><div id = 'default' /></div></div>"
      ],
      "text/plain": [
       "<Result 'C:\\\\Users\\\\thoward3\\\\AppData\\\\Local\\\\Temp\\\\ArcGISProTemp9828\\\\tmplyhequtm\\\\temp.gdb'>"
      ]
     },
     "execution_count": 267,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Clean up temporary files\n",
    "arcpy.management.Delete(\"point_layer\")\n",
    "arcpy.management.Delete(temp_gdb)\n",
    "# os.rmdir(temp_dir)\n",
    "\n",
    "# Load the point feature layer\n",
    "input_point_layer = arcpy.management.MakeFeatureLayer(point_layer_path, \"point_layer\")\n",
    "\n",
    "# Create a temporary workspace\n",
    "temp_dir = tempfile.mkdtemp()\n",
    "temp_gdb = os.path.join(temp_dir, 'temp.gdb')\n",
    "arcpy.management.CreateFileGDB(temp_dir, 'temp.gdb')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: PLDPLD_lakes_pfaf_71\n",
      "Processing complete. Output saved at:  D:\\__THOWARD\\ABOVE_Project\\Technical\\OutputData\\Vectors\\PointFeatureSpatialJoins\\effluxlakes_spatialjoins_2024-07-28.shp\n",
      "Processing: PLDPLD_lakes_pfaf_72\n",
      "Processing complete. Output saved at:  D:\\__THOWARD\\ABOVE_Project\\Technical\\OutputData\\Vectors\\PointFeatureSpatialJoins\\effluxlakes_spatialjoins_2024-07-28.shp\n",
      "Processing: PLDPLD_lakes_pfaf_74\n",
      "Processing complete. Output saved at:  D:\\__THOWARD\\ABOVE_Project\\Technical\\OutputData\\Vectors\\PointFeatureSpatialJoins\\effluxlakes_spatialjoins_2024-07-28.shp\n",
      "Processing: PLDPLD_lakes_pfaf_78\n",
      "Processing complete. Output saved at:  D:\\__THOWARD\\ABOVE_Project\\Technical\\OutputData\\Vectors\\PointFeatureSpatialJoins\\effluxlakes_spatialjoins_2024-07-28.shp\n",
      "Processing: PLDPLD_lakes_pfaf_81\n",
      "Processing complete. Output saved at:  D:\\__THOWARD\\ABOVE_Project\\Technical\\OutputData\\Vectors\\PointFeatureSpatialJoins\\effluxlakes_spatialjoins_2024-07-28.shp\n",
      "Processing: PLDPLD_lakes_pfaf_82\n",
      "Processing complete. Output saved at:  D:\\__THOWARD\\ABOVE_Project\\Technical\\OutputData\\Vectors\\PointFeatureSpatialJoins\\effluxlakes_spatialjoins_2024-07-28.shp\n",
      "Processing: PLDPLD_lakes_pfaf_83\n",
      "Processing complete. Output saved at:  D:\\__THOWARD\\ABOVE_Project\\Technical\\OutputData\\Vectors\\PointFeatureSpatialJoins\\effluxlakes_spatialjoins_2024-07-28.shp\n",
      "Processing: PLDPLD_lakes_pfaf_84\n",
      "Processing complete. Output saved at:  D:\\__THOWARD\\ABOVE_Project\\Technical\\OutputData\\Vectors\\PointFeatureSpatialJoins\\effluxlakes_spatialjoins_2024-07-28.shp\n",
      "Processing: PLDPLD_lakes_pfaf_85\n",
      "Processing complete. Output saved at:  D:\\__THOWARD\\ABOVE_Project\\Technical\\OutputData\\Vectors\\PointFeatureSpatialJoins\\effluxlakes_spatialjoins_2024-07-28.shp\n",
      "Processing: PLDPLD_lakes_pfaf_86\n",
      "Processing complete. Output saved at:  D:\\__THOWARD\\ABOVE_Project\\Technical\\OutputData\\Vectors\\PointFeatureSpatialJoins\\effluxlakes_spatialjoins_2024-07-28.shp\n",
      "Processing: PLDPLD_lakes_pfaf_91\n",
      "Processing complete. Output saved at:  D:\\__THOWARD\\ABOVE_Project\\Technical\\OutputData\\Vectors\\PointFeatureSpatialJoins\\effluxlakes_spatialjoins_2024-07-28.shp\n"
     ]
    }
   ],
   "source": [
    "## EDK in a notebook, can add cell magic %%time if you want to time this operation.\n",
    "# Create a dictionary to store results for each point\n",
    "point_intersections = {}\n",
    "\n",
    "# Execute the function and update output point layer\n",
    "## EDK: can use tqdm to give you a progress bar: for input_polygon_layer in tqdm.tqdm(PLD_shapefile_list): ...\n",
    "for input_polygon_layer in PLD_shapefile_list:\n",
    "    point_intersections = execute_spatial_joins(\n",
    "                              point_layer = input_point_layer, \n",
    "                              input_wbd_layer = input_polygon_layer,\n",
    "                              source = \"PLD\", \n",
    "                              lake_id_field_name = \"lake_id\", \n",
    "                              search_radius_meters = 100, \n",
    "                              output_point_layer_path = output_point_layer_path\n",
    "                             )\n",
    "\n",
    "    with arcpy.da.UpdateCursor(output_point_layer_path, [\"FID\", \"PLDIntNum\",\"PLDIntIDs\", \"PLD100mNum\", \"PLD100mIDs\", \"PLDNdistID\", \"PLDNdist_m\"]) as cursor:\n",
    "        for row in cursor:\n",
    "            fid = row[0]\n",
    "            if fid in point_intersections:\n",
    "                row[1] = point_intersections[fid][\"count\"]\n",
    "                row[2] = ','.join(point_intersections[fid][\"ids\"])\n",
    "                row[3] = point_intersections[fid][\"dist_count\"]\n",
    "                row[4] = ','.join(point_intersections[fid][\"dist_ids\"])\n",
    "                row[5] = ','.join(point_intersections[fid][\"near_id\"])\n",
    "                row[6] = ','.join(point_intersections[fid][\"ndist_m\"])\n",
    "\n",
    "            else:\n",
    "                row[1] = 0\n",
    "                row[2] = \"\"\n",
    "                row[3] = 0\n",
    "                row[4] = \"\"\n",
    "                row[5] = \"\"\n",
    "                row[6] = \"\"\n",
    "            cursor.updateRow(row)\n",
    "\n",
    "    print(\"Processing complete. Output saved at: \", output_point_layer_path) ## EDK: If I understand correctly, this appends to the existing file. This is good, because it saves your work if the code crashes.\n",
    "        \n",
    "# Clean up temporary files\n",
    "arcpy.management.Delete(temp_gdb)\n",
    "os.rmdir(temp_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conduct Spatial Joins with SUI Lakes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class='gpresult'><h2>Messages</h2><div id='messages' data-messages='[\"Start Time: Sunday, July 28, 2024 7:26:05 PM\",\"Adding SuiNdist_m to effluxlakes_spatialjoins_2024-07-28...\",\"Succeeded at Sunday, July 28, 2024 7:26:06 PM (Elapsed Time: 0.21 seconds)\"]' data-show='true'><div id = 'default' /></div></div>"
      ],
      "text/plain": [
       "<Result 'D:\\\\__THOWARD\\\\ABOVE_Project\\\\Technical\\\\OutputData\\\\Vectors\\\\PointFeatureSpatialJoins\\\\effluxlakes_spatialjoins_2024-07-28.shp'>"
      ]
     },
     "execution_count": 269,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Path to the SUI lake polygon feature layer\n",
    "sui_WBD_layer = os.path.join(basepath, r\"InputData\\Databases\\water_edited.gdb\\WBD\")\n",
    "sui_layer_list = [sui_WBD_layer]\n",
    "\n",
    "# Add new columns to the point layer for SUI\n",
    "arcpy.management.AddField(output_point_layer_path, 'SuiIntNum', 'LONG')\n",
    "arcpy.management.AddField(output_point_layer_path, 'SuiIntIDs', 'TEXT', field_length=500)\n",
    "arcpy.management.AddField(output_point_layer_path, 'Sui100mNum', 'LONG')\n",
    "arcpy.management.AddField(output_point_layer_path, 'Sui100mIDs', 'TEXT', field_length=500)\n",
    "arcpy.management.AddField(output_point_layer_path, 'SuiNdistID', 'TEXT', field_length=500)\n",
    "arcpy.management.AddField(output_point_layer_path, 'SuiNdist_m', 'TEXT', field_length=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class='gpresult'><h2>Messages</h2><div id='messages' data-messages='[\"Start Time: Sunday, July 28, 2024 7:26:06 PM\",\"Succeeded at Sunday, July 28, 2024 7:26:06 PM (Elapsed Time: 0.27 seconds)\"]' data-show='true'><div id = 'default' /></div></div>"
      ],
      "text/plain": [
       "<Result 'C:\\\\Users\\\\thoward3\\\\AppData\\\\Local\\\\Temp\\\\ArcGISProTemp9828\\\\tmp4uw35i5i\\\\temp.gdb'>"
      ]
     },
     "execution_count": 270,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Clean up temporary files\n",
    "arcpy.management.Delete(\"point_layer\")\n",
    "\n",
    "# Load the point feature layer\n",
    "input_point_layer = arcpy.management.MakeFeatureLayer(point_layer_path, \"point_layer\")\n",
    "\n",
    "# Create a temporary workspace\n",
    "temp_dir = tempfile.mkdtemp()\n",
    "temp_gdb = os.path.join(temp_dir, 'temp.gdb')\n",
    "arcpy.management.CreateFileGDB(temp_dir, 'temp.gdb')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: SuiWBD\n",
      "Processing complete. Output saved at:  D:\\__THOWARD\\ABOVE_Project\\Technical\\OutputData\\Vectors\\PointFeatureSpatialJoins\\effluxlakes_spatialjoins_2024-07-28.shp\n"
     ]
    }
   ],
   "source": [
    "# Create a dictionary to store results for each point\n",
    "point_intersections = {}\n",
    "\n",
    "# Execute the function and update output point layer\n",
    "for input_polygon_layer in sui_layer_list:\n",
    "    point_intersections = execute_spatial_joins(\n",
    "                              point_layer = input_point_layer, \n",
    "                              input_wbd_layer = input_polygon_layer,\n",
    "                              source = \"Sui\", \n",
    "                              lake_id_field_name = \"SUI_ID\", \n",
    "                              search_radius_meters = 100, \n",
    "                              output_point_layer_path = output_point_layer_path\n",
    "                             )\n",
    "\n",
    "    # EDK: This text can also be generalized to a function to avoid repeating similar text blocks. YOu can generate field names using f-strings: wdb_name='Sui'; f\"{wdb_name}IntNum\", f\"{wdb_name}IntIDs\",] etc.\n",
    "    with arcpy.da.UpdateCursor(output_point_layer_path, [\"FID\", \"SuiIntNum\", \"SuiIntIDs\", \"Sui100mNum\", \"Sui100mIDs\", \"SuiNdistID\", \"SuiNdist_m\"]) as cursor:\n",
    "        for row in cursor:\n",
    "            fid = row[0]\n",
    "            if fid in point_intersections:\n",
    "                row[1] = point_intersections[fid][\"count\"]\n",
    "                row[2] = ','.join(point_intersections[fid][\"ids\"])\n",
    "                row[3] = point_intersections[fid][\"dist_count\"]\n",
    "                row[4] = ','.join(point_intersections[fid][\"dist_ids\"])\n",
    "                row[5] = ','.join(point_intersections[fid][\"near_id\"])\n",
    "                row[6] = ','.join(point_intersections[fid][\"ndist_m\"])\n",
    "\n",
    "            else:\n",
    "                row[1] = 0\n",
    "                row[2] = \"\"\n",
    "                row[3] = 0\n",
    "                row[4] = \"\"\n",
    "                row[5] = \"\"\n",
    "                row[6] = \"\"\n",
    "            cursor.updateRow(row)\n",
    "\n",
    "    print(\"Processing complete. Output saved at: \", output_point_layer_path)\n",
    "        \n",
    "# Clean up temporary files\n",
    "arcpy.management.Delete(temp_gdb)\n",
    "os.rmdir(temp_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New field added and updated successfully.\n"
     ]
    }
   ],
   "source": [
    "# Add the new field to the shapefile\n",
    "new_field_name = \"IntDescrpt\"\n",
    "arcpy.management.AddField(output_point_layer_path, new_field_name, \"TEXT\", field_length=500)\n",
    "\n",
    "# Update the new field based on the conditions\n",
    "with arcpy.da.UpdateCursor(output_point_layer_path, [\"PLDIntNum\", \"SuiIntNum\", \"PLD100mNum\", \"Sui100mNum\", new_field_name]) as cursor:\n",
    "    for row in cursor:\n",
    "        PLDIntNum = row[0]\n",
    "        SuiIntNum = row[1]\n",
    "        PLD100mNum = row[2]\n",
    "        Sui100mNum = row[3]\n",
    "        \n",
    "        if PLDIntNum > 1 and SuiIntNum > 1:\n",
    "            row[4] = \"Intersects multiple lakes in both PLD and Sui\" # EDK: Love that you made this proof by writing out the results in plain English! It's easy for me to get confused when working with tabular data.\n",
    "        elif PLDIntNum > 1 and SuiIntNum < 1:\n",
    "            row[4] = \"Intersects multiple lakes in PLD but not in Sui\"\n",
    "        elif PLDIntNum < 1 and SuiIntNum > 1:\n",
    "            row[4] = \"Intersects multiple lakes in Sui but not in PLD\"\n",
    "        elif PLDIntNum == 1 and SuiIntNum == 1:\n",
    "            row[4] = \"Intersects lake in PLD and Sui\"\n",
    "        elif PLDIntNum == 1 and SuiIntNum == 0:\n",
    "            row[4] = \"Intersects lake in PLD but not in Sui\"\n",
    "        elif PLDIntNum == 0 and SuiIntNum == 1:\n",
    "            row[4] = \"Intersects lake in Sui but not in PLD\"\n",
    "        elif PLDIntNum == 0 and SuiIntNum == 0 and PLD100mNum > 0 and Sui100mNum > 0:\n",
    "            row[4] = \"Within 100 m of lakes in PLD and in Sui\"\n",
    "        elif PLDIntNum == 0 and SuiIntNum == 0 and PLD100mNum > 0 and Sui100mNum == 0:\n",
    "            row[4] = \"Within 100 m of lakes in PLD but not in Sui\"\n",
    "        elif PLDIntNum == 0 and SuiIntNum == 0 and PLD100mNum == 0 and Sui100mNum > 0:\n",
    "            row[4] = \"Within 100 m of lakes in Sui but not in PLD\"\n",
    "        elif PLDIntNum == 0 and SuiIntNum == 0 and PLD100mNum == 0 and Sui100mNum == 0:\n",
    "            row[4] = \"Not within 100 m of previously identified waterbody\"\n",
    "        cursor.updateRow(row)\n",
    "\n",
    "print(\"New field added and updated successfully.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ArcGISPro",
   "language": "Python",
   "name": "python3"
  },
  "language_info": {
   "file_extension": ".py",
   "name": "python",
   "version": "3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
